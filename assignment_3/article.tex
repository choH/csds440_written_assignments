
\documentclass[12pt]{article}
\usepackage{times}
\usepackage{setspace}
\setstretch{1.5}
\usepackage{amsmath,amssymb, amsthm}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[hang, flushmargin]{footmisc}
\usepackage[colorlinks=true]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{footnotebackref}
\usepackage{url}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{inconsolata}
\usepackage[papersize={8.5in,11in}, margin=1in]{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{esint}
\usepackage{url}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{wasysym}
\newcommand{\ilcode}{\texttt}
\usepackage{etoolbox}
\usepackage{physics}
\usepackage{xcolor}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\makeatletter
\renewcommand{\@seccntformat}[1]{}
\makeatother

\begin{document}



\title{\textbf{CSDS 440: Assignment 3}}

\author{Shaochen (Henry) ZHONG, \ilcode{sxz517} \\ Mingyang TIE, \ilcode{mxt497}}
\date{Due on 09/25/2020, submitted \textcolor{blue}{early} on 09/18/2020}
\maketitle


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Problem 10}

It depends on the task and how is the performance of the model being ``worsen'' on test data than training data. Say we have a task to detect fire of a building so that less people get hurt, and the model performance is lower on test data than training data due to having a lot of false positives. Such model may still be beneficial as the cost of having a false negative is a lot more expensive than having a false positive in this particular task. And even though the model might be overfitting by definition, it might perform better than a model that is less overfit but has more false negative.

The other possible scenario include but not limited to:
\begin{itemize}
    \item When we want to test the upper capacity of our model or we plan to compare the fitting capacity of some certain kinds of models, we usually keep models overfit for the data.
    \item When there is no noise in the data, we want our model to fit the pure data as precisely as possible. Under this circumstance, overfit is beneficial. For example, using least square method to fit the curve of an ideal polynomial.
\end{itemize}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Problem 11}

A preference bias is an inductive bias where some hypothesis are preferred over others.

\begin{itemize}
    \item Pros: By using prior knowledge, it allows the learner to work within a complete hypothesis space that is assured to contain the unknown target function.
    \item Cons: We must have correct prior knowledge about hypothesis space. Otherwise, we may miss the target function.
\end{itemize}



\noindent A restriction bias is an inductive bias where the set of hypothesis considered is restricted to a smaller set.

\begin{itemize}
    \item Pros: By restrict hypothesis space, we may need less time and effort to search entire hypothesis space.
    \item Cons: It strictly limits the set of potential hypotheses, which may bring the possibility of excluding the unknown target function altogether.
\end{itemize}




% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Problem 12}

No. This is not a goth methodology due to an effective concept usually takes a reasonably large amount of training to learn. However by having an equal-size training and evaluation sets, the training set is likely not large enough -- or at least not as effective having a larger training set.\newline

Also because of the equal-sized division, the examples in the training set of during an iteration can be very different to another iteration. This inconsistency will increase the difficuty for person \textit{X} to analyse wheather it is the problem on training data or the model itself, should there ever be any undesired/unstable performance measures.

At the same time, some examples might always being divided to the training test during each iterations, so the model will therefore never be able to evaluate its ablity of predicting these examples; and this for sure lower the realiability of the performance measure of the model. In general, a $N$-fold approach will be preferred.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Problem 13}

Because ROC graph is ploting $\text{TP Rate} = \frac{TP}{TP + FN}$ against $\text{FP Rate} = \frac{FP}{FP + TN}$. As we are lowering the classification threshold, more examples will be classfied as \textit{Positive}. This implies there will be more $TP$ and $FP$ (for a typical model) as the threashold being lower -- and since both $TP + FN$ and $FP + TN$ are constant for all time -- we will have a larger numerators on the same denominators. Which will result in an increase on both axises and the statament is therefore proven.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Problem 14}

Let $R$ denotes the examples that are being classfied as \textit{Positive}, and $T$ denotes the true posive cases. We have:

\begin{align*}
    P(R) &= P(R \mid T)P(T) + P(R \mid T^c) P(T^c) \\
    &= P(R \mid T)(1 - P(T^c) + P(R \mid T^c) P(T^c) \\
    &= P(R \mid T) - P(R \mid T)P(T^c)+ P(R \mid T^c) P(T^c) \\
    &= P(R \mid T) + [P(R \mid T^c) -  P(R \mid T)] \cdot P(T^c) \\
    P(R) - P(R \mid T) &= [P(R \mid T^c) -  P(R \mid T)] \cdot P(T^c)
\end{align*}

We know that there must be $P(R) = P(R \mid T)$ as random guessing is an independent variable. We also know we should have $P(T^c) > 0$ for being a meaningful task. Subsituting these into the above equation, we have $0 = P(R \mid T^c) -  P(R \mid T) \Longrightarrow  P(R \mid T^c) = P(R \mid T)$. This implies that the TP Rate is the same as the FP Rate, and the ROC graph for a random gussing classifier will therefore be a diagonal line.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% \section{References}
% \nocite{*}
% \raggedright
% \bibliography{references.bib}
% \bibliographystyle{plain}


\end{document}